# General Parameters, see comment for each definition
# choose the booster, can be gbtree or gblinear
booster = gbtree
# choose logistic regression loss function for binary classification
objective = binary:logistic

# objective="rank:pairwise"
# Tree Booster Parameters
# step size shrinkage
eta = 0.1

# minimum loss reduction required to make a further partition
gamma = 30.0
# 30.0, [91]	test-auc:0.832941	test-logloss:0.071257	train-auc:0.868740	train-logloss:0.067335
# 20 [112]	test-auc:0.831920	test-logloss:0.071215	train-auc:0.910576	train-logloss:0.059959

# alpha = 30.0
# lambea = 10
# minimum sum of instance weight(hessian) needed in a child

min_child_weight = 3
# 2 [89]	test-auc:0.827846	test-logloss:0.071639	train-auc:0.868705	train-logloss:0.067449
# 3 [87]	test-auc:0.833614	test-logloss:0.071342	train-auc:0.864078	train-logloss:0.068016
# 10 [94]	test-auc:0.832151	test-logloss:0.071394	train-auc:0.859601	train-logloss:0.068906

max_delta_step = 1
# [96]	test-auc:0.833052	train-auc:0.856778
# subsample = 0.8
# colsample_bytree = 0.5
# scale_pos_weight = 60.0

# maximum depth of a tree
max_depth = 10
# 12, [86] test-auc:0.830814       test-logloss:0.071330   train-auc:0.977508      train-logloss:0.043388

# Task Parameters
# the number of round to do boosting
# num_round = 20
num_round = 100

# 0 means do not save any model except the final round model
save_period = 0

# The path of training data
data = "data/tr.xgb"
# data = "data/all.xgb"

# The path of validation data, used to monitor training process, here [test] sets name of the validation set
eval[test] = "data/te.xgb"
# evaluate on training data as well each round
eval_train = 1
eval_metric = "auc"
# eval_metric = "logloss"

# The path of test data
test:data = "data/te.xgb"

nthread = 32