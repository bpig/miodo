# General Parameters, see comment for each definition
# choose the booster, can be gbtree or gblinear
booster = gbtree
# choose logistic regression loss function for binary classification
objective = binary:logistic
# objective = binary:logitraw

# objective="rank:pairwise"
# Tree Booster Parameters
# step size shrinkage
eta = 0.1

# minimum loss reduction required to make a further partition
gamma = 30.0

# alpha = 30.0
# lambea = 10
# minimum sum of instance weight(hessian) needed in a child

min_child_weight = 1
# 1 [332]	test-auc:0.840663	train-auc:0.866932

max_delta_step = 1
# [96]	test-auc:0.833052	train-auc:0.856778
# subsample = 0.8
# colsample_bytree = 0.5
scale_pos_weight = 60.0

# maximum depth of a tree
max_depth = 2
# 3 [224]	test-auc:0.837610	train-auc:0.885214
# 2 [355]	test-auc:0.840828	train-auc:0.869452
# 2 [433]	test-auc:0.842120	train-auc:0.877190

# Task Parameters
# the number of round to do boosting
num_round = 450

# 0 means do not save any model except the final round model
save_period = 0

# The path of training data
# data = "data/tr.xgb"
data = "data/all.xgb"

# The path of validation data, used to monitor training process, here [test] sets name of the validation set
eval[test] = "data/te.xgb"
# evaluate on training data as well each round
eval_train = 0
eval_metric = "auc"
# eval_metric = "logloss"

# The path of test data
test:data = "data/te.xgb"

nthread = 32